{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":"<p>Background to General Computing</p> <p>My notes do not have prerequisites, you may not have previously studied computer architecture, or programming. You may not have any theoretical background in operating systems. I intend to lightly cover these topics here.</p> <p>Assisted computation has existed at least from the time of the abacus, but until the era around the second world war, the utility of devices was restricted by the available technologies [1]. This did not prevent advances in the underlying mathematics; names like Boole, Babbage, Lovelace, Napier, Turing, Shannon and Von Neuman are known even to the public and no further examination of their contributions will be made here. From the twentieth century, there was a rapid development in the technologies used for data communications, data storage and processing. Mechanical technologies were used for computing and automation, with relays (an electrical switches) being common in both. Mechanical computing gave way to the first generation of electronic computers based on vacuum tubes or thermionic valves [2]. Preventive maintenance schemes were developed on an ad-hoc basis to keep these valve systems operational, and the first generation of commercial computers were delivered in the early 1950s.</p> <p>The development of solid-state devices such as the transistor in 1947 [3] led to the second generation of computing technology. Transistors were smaller and required less power and as manufacturing techniques improved, so did individual component service lifetime. Military specification transistors may have had mean time between failure (MTBF) of &gt;10^9 hours at 50\u2070C and this figure of 10^9 hours for transistors still shows in many textbooks as a rule of thumb. By the early 1950s, the first transistor computers were being developed at the University of Manchester [4] and commercial computers were introduced in the late 1950s. These were mainframes and (much later models) are still found today in banking, insurance and large-scale manufacturing.</p> <p>As early as 1949, Werner Jacobi of Siemens AG filed a patent [5] for an amplifier with three transistors, the first integrated circuit (IC). Later work by Jack Kilby of Texas Instruments [6] created an integrated oscillator circuit; Kilby was awarded the Nobel prize in physics in 2000 for his contributions. If ICs were produced at volume, the cost per transistor, size and power utilization all dropped, but so did circuit complexity and assembly effort. The introduction of ICs is typically used to define the third generation of computing hardware in the 1960s. Minicomputers emerged and became ubiquitous in even small universities. This availability of computers led to an increase in the pace of development across many related fields. I first worked on an Apple II around 1978! As each new generation of computer emerged (VIC20, Commodore 64, Spectrum) I learned a little bit more. My first work with mini-computers was with Honeywell Level 6 minicomputers; the Central Processing Unit (CPU) was a pizza-box sized motherboard using ICs. The OS was configured using automation (!) and was built using a Multics mainframe.</p> <p>The ubiquity of computers is something that only emerged in the past generation, and I would argue that the entire field remains immature, we have much to learn.</p> <p>[1] M. Guarnieri, \"The Roots of Automation Before Mechatronics [Historical],\" IEEE Industrial Electronics Magazine, vol. 4, no. 2, pp. 42-43, 2010, doi: 10.1109/MIE.2010.936772. [2] G. R. Jessop, \"Developments in thermionic valves during the last sixty years,\" Journal of the Institution of Electronic and Radio Engineers, vol. 57, no. 2, pp. 81-90, 1987, doi: 10.1049/jiere.1987.0034. [3] \"The transistor \u2014 A new amplifier,\" Electrical Engineering, vol. 67, no. 8, pp. 740-740, 1948, doi: 10.1109/EE.1948.6444253. [4] \"Transistor computers,\" Journal of the Institution of Electrical Engineers, vol. 2, no. 21, pp. 542-543, 1956, doi: 10.1049/jiee-3.1956.0240. [5] W. Jacobi, \"Halbleiterverst\u00e4rker,\" Germany, 1952. [ ] J. Kilby, \"Miniaturized Electronic Circuits,\" USA, 1964. [6] J. Kilby, \"Miniaturized Electronic Circuits,\" USA, 1964.</p>"},{"location":"Hardware/a/","title":"Components","text":"<p>Modern devices abstract way thier internal complexity. I want to try to explain how a computer works from the component level upwards.  These notes are extracted from undergraduate Computer Architecture material I wrote c. 2014. </p> <p>The fundamental components we use in every electrical circuit include:</p>"},{"location":"Hardware/a/#resistors","title":"Resistors","text":"<p>Resistors are two-terminal devices that restrict, or resist, the flow of current. The larger the resistor the less current can flow through it for a given voltage as demonstrated by Ohm\u2019s law: V= I*R. </p> <p>Electrons flowing through a resistor collide with material in the resistor body, and it is these collisions that cause electrical resistance. These collisions cause energy to be dissipated in the form of heat or light (as in a toaster or an old incandescent light bulb). </p> <p>Resistance is measured in Ohms (\u03a9) and an ohm is defined by the amount of resistance that causes 1A of current to flow from a 1V source. </p> <p>The amount of power (in Watts) dissipated in a resistor can be calculated using the equation P= I*V = I<sup>2</sup>R A resistor that can dissipate about 5 Watts of power would be about the size of a pen and a resistor that can only dissipate 1/8 Watt is about the size of a grain of rice. </p> <p>If a resistor is placed in a circuit where it must dissipate more that its intended power, it will melt!</p> <p>A standard colour coding scheme exists for resistors and any experienced electronics person can tell the value of a resistor at a glance! However, as the number of colur bands can vary, it\u2019s always better to check with a digital volt meter (DVM).</p> <p>Look up resistor colour coding now.</p>"},{"location":"Hardware/a/#capacitors","title":"Capacitors","text":"<p>A capacitor (called a condenser in the old days!) is a passive two-terminal device that can store electric energy in the form of charged particles. A capacitor is like a reservoir of charge that takes time to fill and empty. </p> <p>The construction of capacitors is more varied than that or resistors, but the general principle is that two conductive plates are separated by is non-conductive dielectric.</p> <p>When there is a potential difference (voltage) across the conductors, a static electric field develops across the dielectric, causing positive charge to collect on one plate and negative charge on the other plate. </p> <p>Energy is stored in the electrostatic field.</p> <p>The voltage across a capacitor is proportional to the amount of charge it is storing, the more charge added to a capacitor of a given size, the larger the voltage across the capacitor. It is not possible to instantaneously move charge to or from a capacitor, so it is not possible to instantaneously change the voltage across a capacitor. It is this property that makes capacitors useful in many applications.</p> <p>Capacitance is measured in Farads. A one Farad capacitor can store one Coulomb of charge at one volt. For engineering on a small scale (i.e., hand-held or desk-top devices), a one Farad capacitor stores far too much charge to be of general use (it would be like a car having a 1000 gallon petrol tank). </p> <p>More useful capacitors are measured in micro-farads (uF) or pico-farads (pF). The terms \"milli-farad\u201c and \"nano-farad\" are rarely used. Large capacitors often have their value printed plainly on them, such as \"10 uF\" (for 10 microfards).</p>"},{"location":"Hardware/a/#inductors","title":"Inductors","text":"<p>An inductor (also called a choke or coil) is a passive two-terminal electrical device that stores energy. Although this sounds a bit like a capacitor, it is different in that a capacitor stores energy in an electric field, an inductor stores it in its magnetic field.  In practical terms, inductance is the characteristic of an electrical circuit that opposes the starting, stopping, or a change in value of current. Even a perfectly straight length of wire has some inductance. Current flowing in a conductor produces a magnetic field surrounding the conductor; when the current changes, the magnetic field changes. This causes a relative motion between the magnetic field and the conductor, and a back electromotive force (EMF) is induced in the conductor. The polarity of the back electromotive force is in the opposite direction to the applied voltage of the conductor. The overall effect will be to oppose a change in current magnitude. Some effects we may notice are that the start-up current drawn by an electric motor is much higher than the current required to run the motor (the coil has to \u201ccharge\u201d).   </p> <p>The symbol for inductance is L and the basic unit of inductance is the HENRY (H). To quantify the unit, an inductor with an inductance of 1H produces an EMF of 1V when the current through the inductor changes at the rate of 1A per second.</p> <p>In appearance, an inductor can be as simple as a coil of copper wire. They can be cylindrical-shaped or torus shaped (the engineer\u2019s term for shaped like a doughnut!)</p> <p>More commonly, an inductor will have a core of a ferromagnetic material.  The core material has the effect of increasing the magnitude of the magnetic field and reducing its physical size.</p>"},{"location":"Hardware/a/#diodes","title":"Diodes","text":"<p>Very frequently we need electronic components which can act like one way valves; current will only pass one way through the device, it will not pass back. These devices are called diodes. They are two terminal devices and are used for a range of purposes. They consist of two layers of a semiconductor (normally silicon) sandwiched together. </p> <p>Almost every power supply will have a few diodes combined with a transformer and a few capacitors.</p> <p>One version of the diode is the LED or light emitting diode. A property of the joint between the two layers causes light to be emitted. </p>"},{"location":"Hardware/a/#transistors","title":"Transistors","text":"<p>A transistor is an active electronic device with (at least) three terminals. They generally consist of three layers of a semiconductor (normally silicon) sandwiched together. </p> <p>A transistor can act like an amplifier. A voltage or current applied to an input control terminal to change the current flowing through another pair of output terminals; this property is called gain. Because the controlled power can be higher than the controlling power, a transistor can amplify a signal. This is the basis for the audio subsystems of all amplifiers, stereo systems, iPods, etc.</p> <p>A transistor can act also like a switch. When no voltage is applied to an input control terminal, no current flows through another pair of output terminals. Alternatively, when a voltage over a particular threshold is applied to the input control terminal, then current flows through the output terminals.  This is the basis for digital logic and all modern binary computers.</p> <p>Because the controlled power can be higher than the controlling power, a transistor can switch a large current using only a small controlling current. This is the basis of all power electronics.</p> <p>The original transistors we used were called bipolar transistors. </p> <p>These are three terminal active devices that can conduct current between two terminals (the collector and the emitter) when a third terminal (the base) is driven by an appropriate signal.</p> <p>The transistor switches used in modern digital circuits are called \u201cMetal Oxide Semiconductor Field Effect Transistors\u201d, or MOSFETs or just FETs. FETs are also three terminal devices that can conduct current between two terminals (the source and the drain) when a third terminal (the gate) is driven by an appropriate logic signal. FETs can be thought as electrically controllable ON/OFF switches</p>"},{"location":"Hardware/b/","title":"Logic Gates","text":"<p>Once we have basic components, we can build for complex circuits, logic gates.</p> <p>The study of logic goes back (at least!) to our earliest cultural origins; the Arabs, Greeks, Chinese and Indians all had advanced concepts which were the origin of our modern science.  In modern computer science and mathematics, we trace our reasoning back to Aristotle and the early Greek mathematicians, as we strove to formally describe reasoning, arguments and proofs in an unambiguous mathematical language. The term logic itself comes from the Greek \u03bb\u03bf\u03b3\u03b9\u03ba\u03ae logik\u0113 or the study or arguments. </p> <p>Some of the most important work in logic (in fact the basis of modern computer theory) was carried out by George Boole at UCC from 1849. Boole established the system now known as Boolean algebra, where variables are truth values true or false. The device which we based our electronic revolution on was the transistor; electronic switches which could have the values on or off. Boolean algebra was ideally suited to this binary environment has been fundamental to our development of digital logic, modern electronics, and eventually computing. In the 1930\u2019s, Claude Shannon applied Boolean algebra to circuits built using switches, providing the foundations for modern computing.</p> <p>Boolean expressions are normally true or false, or on and off, or 1 and 0, all are representations of the same thing. In any system, we can have Boolean variables which can contain a Boolean expression. </p> <p>Boolean operators let us look at the relationship between the state of these variables and an output. Let\u2019s go with variable C for an output.  We know that if A is on (true) and B is on (true) then the machine is safe and C should be equal to on (true) But look at all the possible cases that give rise to an output. We call this a truth table; it allows us to look at all possible inputs and have a predictable and desired output. </p> Fig 1. An AND gate, truth table and symbol. <p>In logic terms, IF A AND B THEN C. </p> <p>This behaviour is the Boolean operator AND. This is formally represented by the symbol and truth tables shown in Fig 1. By convention, we show our Boolean expressions as 1 and 0, the binary levels used in both digital logic and in computing.</p> <p>The second Boolean operator we will consider is the OR gate, Fig 2. In this case if either input A or input B is equal to 1 (true) then the output of the gate will be 1 (true).</p> <p>The output is 0 (false) only when both inputs are 0 (false).</p> Fig 2. An OR gate, truth table and symbol. <p>In electronics, we may sometimes buffer an input from an output. This is done for a range of reasons, mostly to reduce the electrical load on a circuit.  </p> <p>The triangle symbol on its own means it\u2019s just a buffer, it does not change the Boolean value at its input to anything different at the output.</p> Fig 3. A buffer, truth table and symbol. <p>Sometimes we need a logic function which just inverts a Boolean variable. For example if we have a 1 and we need a 0, or vice versa. We refer to this as a NOT function.</p> <p>We can signify this in a few different ways. We can use any standard logic symbol with a circle on the output. The circle denotes an inversion.  Alternatively, we can put a bar over the variable symbol; a bar over A means NOT A. </p> Fig 4. An inverter, truth table and symbol. <p>We can apply the same sort of approach to other logic gates. For example a NOT AND gate is a NAND gate. It has the same truth table as an AND gate, except with the output inverted. Similarly a NOT OR gate is called a NOR gate.</p> Fig 5. NOR and NAND, truth table and symbol. <p>The final Boolean operator we need to look at is the exclusive OR (XOR) gate. This is a special case or the OR gate where the output is 1 if either input is 1, but is 0 is all inputs are 1. Obviously we can also have an XNOR, which is an XOR with the output inverted. </p> Fig 6. XOR and XNOR, truth table and symbol. <p>In early digital systems (c. 1950) we actually built logic gates out of transistors, resistors and other discrete components. </p> <p>By the 1960\u2019s we had integrated these transistors into single chunks of silicon we called integrated circuits or IC\u2019s. This allowed for the next revolution in computing, which allowed us to get a CPU onto a single large circuit board. These early silicon chips used transistor-transistor logic (TTL) and can be recognised by a 74xxx prefix. </p> <p>For example, a 7400 was a package with 14 legs which had four NAND gates inside.</p> Fig 7. 7400 TTL. <p>In later circuits, the 74xxx series were superceeded by 4xxxx series, using CMOS transistors and lower power.</p> <p>Based on the availability of the early silicon chips, computers began to be built which were commercially viable. The IBM 360 series revolutionised early computing using these kinds of technologies. Throughout the 1970\u2019s mini-computers and mainframes were developed and the computing industry blossomed.</p> <p>I still use these chips when designing and building simple prototypes.</p> <p>Do some background reading and identify some circuits that use transistors to implement logic gates.</p>"},{"location":"Hardware/c/","title":"Memory","text":"<p>In this section, I am going to build up gates into useful subsystems, first memory!</p>"},{"location":"Hardware/c/#the-setrest-latch","title":"The Set/Rest Latch","text":"<p>The first thing that we needed to do useful calculations with these gates was to have a way of storing numbers (remember, all numbers are represented in binary within the computer); this is the concept of memory. We need a circuit which can store a single bit of data; we call a circuit like this a latch. There are several types of latch available in TTL which were built into early computers (and are now integrated into much larger circuits).</p> Fig 8. SR Latch. <p>One of these simple circuits was the set-reset latch or S-R Latch. Very simple, if you raise the input S to a one, Q will set to 1 and \u00afQ will reset to 0. If you raise the R input to 1, Q will reset to 0 and \u00afQ will set to 1. Work your way through the circuit to confirm this!</p> <p>One of the problem with this circuit is that if you raise S and R to 1 at the same time, the circuit acts in an unpredictable manner. Again, try working your way through the circuit to see.</p> <p>Up to now, all the digital gates and circuits we have looked at have been deterministic, that means that for the inputs given, there is only one possible outcome. Once we start dealing with circuits which can store bits (memory) we need to understand the concept of state; a circuit can be in a particular state before we encounter it and the output of any action may depend on the state the circuit was in before we started.</p>"},{"location":"Hardware/c/#clocks-and-timing","title":"Clocks and Timing","text":"<p>In a real computer, most things are synchronised by a timing signal. Have a look at the circuit below where we introduce a timing signal, known as a clock. The AND gates block any inputs from either S or R until the clock signal is high. Now we can control when the set or reset command are sampled.</p> Fig 9. Clocked SR Latch."},{"location":"Hardware/c/#the-d-latch","title":"The D Latch","text":"<p>In a D Latch, we allow only one input, directly to the set AND gate at the top of the diagram, and via an inverter to the reset AND at the bottom of the diagram. When D=1 and the clock signal is high, we set Q=1. When D=0 and the clock signal is high, we reset Q=0. Work your way through the circuit to confirm this. This is a pretty good circuit but there is one issue. The clock pulse is quite wide and the latch can be set at any stage while the pulse is high. This is none too precise, especially when we are trying to get the maximum speed and performance out of our electronics. </p> Fig 10. D Latch."},{"location":"Hardware/c/#edge-detection","title":"Edge Detection","text":"<p>One of the ways we can get best performance out a clocked circuit is to activate the circuit or the edge of the pulse rather than on the pulse itself. Consider the circuit below. Each gate has a propagation delay to activate. Suppose each gate takes 1 ns to activate and a starts as 0. If a is at 0 then b must be at 1 because of the inverter. When the pulse goes high at a it goes high at c at the same time. However, the inverter causes a delay of 1 ns before b becomes 0. For this brief period, both b and c were 1 and the AND gate output becomes 1 for this brief time period. We have detected the rising edge of the pulse.</p> Fig 11. Edge Detection. <p>One other interesting thing we can do here is to detect both the rising and falling edge. That would give us a good narrow pulse at double the rate of the clock. I wonder where we might come across Double Data Rate (DDR) being used. Try an Internet search if you haven\u2019t come across this before.</p> <p>We can use an edge trigger circuit to trigger a D Latch to make it much more precise. In the circuit below, the clock is intercepted by a trigger circuit to make an edge-trigger circuit. A latch which is edge triggered is called a flip-flop.</p> Fig 12. Flip-flop. <p>Flip-flops are very commonly used in all kinds of digital logic circuits. There are several different block circuits used. To keep things simple, we abstract the details away. We can treat a half adder like a black box, not worrying too much about what is inside it. This allows us to simplify our diagrams, getting more and more unnecessary detail out of the way so we can understand more complex circuits more easily.</p> <ul> <li>A D Latch is triggered by a positive clock</li> <li>A D Latch is triggered by a negative clock</li> <li>The triangle indicates a positive edge triggered flip-flop</li> <li>The triangle and circle indicates a negative edge triggered flip-flop</li> </ul> Fig 13. Triggering. <p>By combining four flip-flops in an array, we can create a four bit memory store, a store which could hold half a byte of information. We could use this as memory, or we could use it as a temporary area to store data that we need to process; we call this a register.</p> Fig 14. 4 bit memory. <p>When the circuit starts up, everything is at zero. I set values for D0 to D3 on the input bus. When the values are set and I want to write to the register, I toggle the write switch. This enables the clock through the AND gate and creates an edge to clock the D flip flops. They then store whatever is on the input bus. The output is available on the output bus indefinetely or until it is overwritten or the power is removed. </p> <p>As with all things computing, once we understand how something complex works inside, we build an abstraction to hide the detail. A block diagram of a register might look something like this.</p> Fig 15. 8 bit register. <p>Some registers have a very defined purpose and will have extra functionality built into their logic. For example, there are times when we need sequential counters, these may have functions like increment or decrement built in. Some mathematical manipulations require us to shift the bits in a register left or right, this may also be built in. </p>"},{"location":"Hardware/d/","title":"Calculation","text":"<p>We now know how to do basic digital logic and how to create basic memory elements. The next thing we need to be able to do is to manipulate numbers stored in those memory elements.</p> <p>When we add two binary numbers together, what are the possible outcomes?</p> Table 1. Simple addition. <p>So to add two binary numbers, we will need outputs for a sum and a carry. Let\u2019s do a truth table and figure out how this should work.</p> Table 2. Simple addition, truth table. <p>Make sure this makes sense to you before reading on! Can you figure out what gate matches the sum column\u2026what gate give a 1 output when either input is one, but a 0 output when both gates are 1?  You should know this!  Look at the carry column. What gate give a 1 output only when both inputs are 1?  If you couldn\u2019t figure this out, go back and read the notes on logic gates again.</p> <p>A circuit to perform this simple mathematical function is called a half-adder (for reasons we will see later!). I have built a half-adder circuit to test it. I have used two memory elements (D flip flops) to store my A and B values. I have included the XOR and AND gates and have put in some lights so I can see what is going on. I tested it and it works!</p> fig 16. A half adder. <p>So we have a success\u2026.we can now add any numbers up to\u2026..2! Check closely, we can never have an answer of 3.</p> <p>This isn\u2019t much use. When you did binary in semester 1, you learned that to add two numbers, you need to be able to carry from one stage to the next. In the adjacent sum;</p> <ul> <li>1 + 0 = 1 and no carry</li> <li>1 + 1 + no carry = 0 carry the 1</li> <li>1 + 1 + 1 carry = 1 carry the 1</li> <li>1</li> </ul> <p>Giving an answer of 1101</p> <p>Our half-adder would work fine for the first calculation, where we can only add A and B. However, in the second calculation, we must be able to add A and B and the carry. What would a truth table for this look like?</p> <p>I have built a (slightly more complex!) circuit in Cedar Logic to allow for a carry-in. What I have done in this case is to put two half adders together, one adds the two numbers to get the sum, the other adds the carry in to the sum to get a final sum.  Now we can add 1 + 1 + 1 to get 3!</p> fig 17. A full adder. <p>Take a look at the truth table. </p> Table 3. Full adder addition, truth table. <p>Finally, do a little reading on twos complement in binary, we can use that technique in maths to do subtraction. </p> <p>I have created a simple circuit using four full adders which adds two four bit numbers in the normal way as A + B. However if I turn on the subtract switch, it adds one (by connecting to the carry in) and inverts the B bits giving the sum A-B. </p> fig 18. A four bit adder. <p>By using simple tricks and mathematical techniques, we can do complex manipulation with very simple circuits.</p> <p>This may not seem very impressive, but remeber the first microprocessor was the Intel 4004, a four-bit processor!</p>"},{"location":"Hardware/e/","title":"Processor Architecture","text":""},{"location":"Hardware/g/","title":"Modern CPU Design","text":"<p>Most of your computing experience has probably been on Intel/AMD type processors. As a family, these are Complex Instruction Set Computers (CISC).  The design uses complex hardware to execute complex instructions over many clock cycles. The processors are relativly big and power-hungry.</p>"},{"location":"Hardware/h/","title":"Alternative Processor Paradigms","text":"<p>So far on this module we have looked at the technologies used in workstations, servers and laptops.  For the past forty years, this market has been controlled by Intel and a few additional players like AMD. </p> <p>Reduced Instruction Set Computers (RISC) is a processor design using simple hardware and highly optimized instruction sets. They are faster per instructions, physically smaller and simpler and as a result, more power efficient. </p>"},{"location":"Hardware/h/#risc","title":"RISC","text":"<p>When we look into the actual execution of instructions in a processor, we can identify empirically (by experimentation and real data) which instructions are used most, which take most processor time. A processor is a number cruncher, so you would guess that arithmetic logic instructions would be executed most? </p> <p>Wrong! </p> <p>A processor spends more time shifting data in and out of memory than doing anything else. The second most common thing for it to do is to control the flow of program execution. Optimization techniques such as pipelining and caching are intended to optimize this.  </p> <p>Reduced Instruction Set or RISC processors were first defined in a 1980 paper by Patterson and Ditzel [1] and early experimentation with RISC in Berkeley exposed some of the characteristics of this model.</p> <ul> <li>Processors were kept as simple as possible with fixed instruction sets or fixed length. </li> <li>Instructions which process data only operate on registers, not on memory, speeding up and simplifying processing.</li> <li>There are many registers, typically thirty two. This was far in excess of the handful of registers in a typical CISC processor.</li> <li>Instruction decoding is hard wired. CISC processors were so complex, they required microcode in the core to assist decoding.</li> <li>There is a concentration of optimization strategies like pipelines.</li> </ul> <p>The emergent properties of this strategy were that die sizes were much smaller, with fewer transistors in a smaller silicon chip. This resulted in less power consumption, a shorter development time, and for a range of complex reasons, better performance and reduced costs. \u2003</p>"},{"location":"Hardware/h/#advanced-risc-machines-arm-processors","title":"Advanced RISC Machines (ARM) processors","text":"<p>Acorn Computer Ltd. (Cambridge, England) developed the first commercial RISC chip in the mid-1980s, the ARM processor [2]. They have an interesting business model. They develop the instruction sets, tools and specifications and then license the production of chips to large silicon foundries. </p> <p>ARM is now the most widely used instruction set worldwide. Early chips shipped were 32 bit; but from 2011, a 64 bit version (ARMv8) has been available. The ARM contains all the components for a computer on a single silicon die; it is therefore a System on a Chip or SoC. The instruction set design is what we would expect from RISC. Instructions are simple and most instructions execute in a single clock-cycle. If you want to work with an ARM based system, a Raspberry PI is a cheap and easy solution. You can load a full copy of Linux (the standard is a derivative of Debian Linux). </p> <p>The cost of a board like the Raspberry Pi 5 is about \u20ac50, depending on memory etc.</p>"},{"location":"Hardware/h/#microcontrollers","title":"Microcontrollers","text":"<p>A vast amount of devices need simple, low power (in terms of both compute and consumption) controllers. Since the late 1970s controllers such as the Peripheral Interface Controller (PIC) have been used for embedded systems and small systems control.  A modern PIC will be programmable with on board flash memory. Typically a PIC will be programmed in Basic, C or C++ and will have simple and widely available tools, code examples and application notes. </p> <p>In machine code, there are a very limited number of instructions (40-80) and they are all fixed in length. There is one accumulator register (W0) which holds the results of all the calculations. Oddly, the RAM used for data is also used for storing temporary values etc. This memory is used like it was a series of registers which map into RAM. Memory is mostly 8 bit although this varies with higher end PICs.</p> <p>Programs are stored in a separate memory location, normally in flash RAM, so they are non-volatile. </p> <p>There is a stack, but it is implemented in the hardware, separately.</p> <p>[1] Patterson, D.A. and Ditzel, D.R., 1980. The case for the reduced instruction set computer. ACM SIGARCH Computer Architecture News, 8(6), pp.25-33.</p> <p>[2] Furber, S.B., 2000. ARM system-on-chip architecture. Pearson Education.</p>"},{"location":"Hardware/i/","title":"Graphics Processor Units (GPU)","text":"<p>Original video cards began as devices without onboard calculation capability, beyond simple memory operations.  The arcade game industry pushed the limit of these techniques.</p> <p>The first dedicated display processors began to emerge in the 1980s.</p> <p>From the 1990s, cards were developed to take load off the CPU and perform garphics operations indendently.</p> <p>Graphics Processor Units were deleloped as video cards for complex rendering, typically for game technology. </p>"},{"location":"Hardware/j/","title":"Automation and I/O","text":"<p>As PICs are commonly used for automation, we have a whole new set of terms to describe how to interface with the outside world. We have to be careful here. A PIC will require signals with particular characteristics, such as high and low voltage. The process you are working with (whatever you are automating or controlling) may use completely different signal levels. You may need to come up with conditioning circuitry to interface the two. </p> <p>For example. I like to use Raspberry Pi as a basic controller and Internet of Things (IoT) device. It has a general purpose input/output interfae (GPIO) for digital singals. But it expects +/- 3.3VDC and almost any higher voltage will do physical damage to the board. I cannot connect an RS232 interface without chaning volatge levels. I may need a relay board to switch voltage/power. Look up electromechanical relays, in this context they are logic controlled switches.  </p>"},{"location":"Hardware/j/#digital-input-di","title":"Digital Input (DI)","text":"<p>A binary signal, a high voltage is one and a low voltage is zero. High and low voltage are defined by the circuit, typically 5VDC and 0VDC.</p>"},{"location":"Hardware/j/#digital-output-do","title":"Digital Output (DO)","text":"<p>A binary signal, a high voltage is one and a low voltage is zero. High and low voltage are defined by the circuit, typically 5VDC and 0VDC. The current output is very limited, so if you are switching lights or a motor, you will need relays or additional switching electronics.</p>"},{"location":"Hardware/j/#analogue-input-ai","title":"Analogue Input (AI)","text":"<p>An analogue to digital converter (ADC) take real world variable voltages and converts them into digital signals. The two important points are, how frequently does the ADC sample (for example, a sound card may sample at 44kHz) and how many bits per sample (8 bits per sample would allow for 256 discrete levels).</p>"},{"location":"Hardware/j/#analogue-output-ao","title":"Analogue Output (AO)","text":"<p>A digital to analogue converter (DAC) takes a digital numeric value and converts it into an analogue voltage level. As with analogue inputs, it will do so at a frequency and in certain steps, depending on the resolution of the DAC.</p>"},{"location":"Hardware/j/#pulse-input-pi","title":"Pulse Input (PI)","text":"<p>Many processes emit pulses. For example, one way of measuring speed and distance is to have a magnet at one point on a wheel. As it passes a sensor, the sensor emits a pulse. Counting these pulses in a fixed time (e.g. one second) allows the frequency of revolution of the wheel to be estimated.</p>"},{"location":"Hardware/j/#pulse-output-po","title":"Pulse Output (PO)","text":"<p>Some processes may be controlled by a pulsed output, where either the number of duration of pulses controls an actuator. Stepper motors (used in robotics) are an example of a device which could be controlled by pulses. </p>"}]}